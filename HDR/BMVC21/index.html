<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Few-Shot Domain Adaptation for Low Light RAW Image Enhancement">
  <meta property="og:title" content="FSDA-LL"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/fsdall.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="FSDA-LL">
  <meta name="twitter:description" content="Few-Shot Domain Adaptation for Low Light RAW Image Enhancement">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/fsdall.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Few-Shot Domain Adaptation for Low Light RAW Image Enhancement">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Few-Shot Domain Adaptation for Low Light RAW Image Enhancement</title>
  <link rel="icon" type="image/x-icon" href="static/images/fsdall.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Few-Shot Domain Adaptation for Low Light RAW Image Enhancement</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=gBhmvr8AAAAJ&hl=en" target="">K. Ram Prabhakar</a>,</span>
              <span class="author-block">
                <a href="https://vishal-v.github.io/" target="">Vishal Vinod</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=uoIh14MAAAAJ" target="">Nihar Ranjan Sahoo</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=cVg7HrEAAAAJ&hl=en" target="">R. Venkatesh Babu</a></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Video Analytics Lab, CDS,<br/>Indian Institute of Science (IISc)<br/>BMVC 2021 [<b>Oral, Best Student Paper Award (Runner-Up)</b>]</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2303.15528.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                   <span class="link-block">
                     <a href="https://drive.google.com/file/d/19i8o6I8CyJ5iFlWbjPatNKK57q5jUt64/view?usp=sharing" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                     <span class="icon">
                       <i class="fas fa-file-pdf"></i>
                     </span>
                     <span>Supplementary</span>
                   </a>
                 </span>

                  <!--Data link  -->
                 <span class="link-block">
                     <a href="https://www.kaggle.com/datasets/razorblade/nikon-camera-dataset" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                     <span class="icon">
                       <i class="fas fa-images"></i>
                     </span>
                     <span>Dataset</span>
                   </a>
                 </span>

                  <!-- Github link -->
                 <span class="link-block">
                   <a href="" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                     <i class="fab fa-github"></i>
                   </span>
                   <span>Code</span>
                 </a>
               </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2303.15528" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- <h3 class="title is-4">Texture Transfer</h3> -->
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="results" class="center">
      <h2 class="subtitle has-text-centered">
    </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Enhancing practical low light raw images is a difficult task due to severe noise and color distortions from 
            short exposure time and limited illumination. Despite the success of existing Convolutional Neural Network (CNN) 
            based methods, their performance is not adaptable to different camera domains. In addition, such methods also 
            require large datasets with short-exposure and corresponding long-exposure ground truth raw images for each camera 
            domain, which is tedious to compile. To address this issue, we present a novel few-shot domain adaptation method to 
            utilize the existing source camera labeled data with few labeled samples from the target camera to improve the target 
            domain's enhancement quality in extreme low-light imaging. Our experiments show that only ten or fewer labeled samples 
            from the target camera domain are sufficient to achieve similar or better enhancement performance than training a model 
            with a large labeled target camera dataset. To support research in this direction, we also present a new low-light raw 
            image dataset captured with a Nikon camera, comprising short-exposure and their corresponding long-exposure ground truth 
            images.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <img src="static/images/method.png" alt="teaser" class="center">
        <div class="content has-text-justified">
          With a noisy raw image captured with low-exposure time (i.e., shutter speed) as input, our CNN-based approach is trained to predict a 
          clean long-exposure sRGB output of the same scene. The input is multiplied by an exposure factor calculated by the ratio of output and 
          input exposure times. For example, to generate a 10-second long exposure output, the input 0.1-second low exposure image must be 
          multiplied by 100. As a result of this operation, along with illumination, the noise is also amplified proportionally. Since we multiply 
          the factor in the unprocessed raw domain and expect the output in the sRGB domain, the network must learn camera hardware-specific 
          enhancement as well as its entire ISP pipeline (lens correction, demosaicing, white balancing, color manipulation, tone curve application, 
          color space transform, and Gamma correction). Thus, a model trained on one specific camera data (source domain) does not translate similar 
          performance to a different camera (target domain), hence the domain gap. In this work, we propose to transfer the enhancement task from 
          large labeled source data and generate output in the target domain using few labeled target data.
        <br/>
        </div>
        <h2 class="title is-3">Nikon Dataset</h2>
        <div class="content has-text-justified">
        <img src="static/images/nikon.png" alt="teaser" class="center">
        <!-- <div class="content has-text-justified"> -->
          Example short-exposure and long-exposure image pairs from the Nikon dataset. The short exposure images are almost entirely dark whereas 
          the long-exposure images have immense scene information.
        <br/>
        </div>
        <div class="content has-text-justified">
          We have compiled a dataset of raw low-light images captured with a Nikon D5600 camera to train the proposed few-shot domain adaptation 
          architecture. The Nikon dataset consists of short-exposure images captured at 1/3 or 1/10 seconds and 
          corresponding ground-truth long-exposure images captured at 10 or 30 seconds in the NEF format. For uniformity, there are two short-exposure 
          images for every long-exposure image such that the exposure ratio (ratio of exposure time between the ground-truth long-exposure image and 
          the input short-exposure image) is 100 and 300, respectively. Similar to LSID, we mount the camera on sturdy tripods and 
          use appropriate camera settings to capture the static scenes using a smartphone app. The images captured include 129 short-exposure and 65 
          long-exposure ground-truth images of indoor and outdoor low-light scenes (sub lux).
        <br/>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>

        <h3 class="title is-4">Sony as Source and Nikon as Target</h3>
        <img src="static/images/Nikon-Results-1_1.png" alt="results" class="center">
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Nikon target images. -->
        </div>

        <!-- <img src="static/images/Nikon-2.png" alt="results" class="center">
        <div class="content has-text-centered">
          <br>Qualitative comparison between different methods tested on Nikon target images.
        </div> -->

        <img src="static/images/Nikon-3.png" alt="results" class="center">
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Nikon target images. -->
        </div>

        <br/>

        <h3 class="title is-4">Sony as Source and Canon as Target</h3>
        <img src="static/images/Canon-Results-1.png" alt="results" class="center">
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Canon target images. -->
        </div>

        <!-- <h3 class="title is-4">Canon dataset </h3> -->
        <img src="static/images/Canon-2.png" alt="results" class="center">
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Canon target images. -->
        </div>

        <!-- <img src="static/images/Canon-3.png" alt="results" class="center">
        <div class="content has-text-centered">
          <br>Qualitative comparison between different methods tested on Canon target images.
        </div> -->

        <br/>

        <h3 class="title is-4">Sony as Source and OnePlus 7 as Target</h3>
        <img src="static/images/OnePlus-1.png" alt="results" class="center">
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Canon target images. -->
        </div>

        <br/>

        <h3 class="title is-4">Sony as Source and Google Pixel as Target</h3>
        <img src="static/images/Pixel-1.png" alt="results" class="center">
        <div class="content has-text-centered">
          <!-- <br>Qualitative comparison between different methods tested on Canon target images. -->
        </div>

      </div>
    </div>
  </div>
</section>

<!-- <section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div id="wrapper">
        <video id="home1" poster="" autoplay controls muted loop>
        <source type="video/mp4" src="static/videos/id_11.mp4" />
        </video>
        <video id="home2" poster="" autoplay controls muted loop >
        <source type="video/mp4" src="static/videos/id_11_edit.mp4" />
        </video>
        <video id="home3" poster="" autoplay controls muted loop >
        <source type="video/mp4" src="static/videos/id_124.mp4" />
        </video>
        <video id="home4" poster="" autoplay controls muted loop >
        <source type="video/mp4" src="static/videos/id_124_edit.mp4" />
        </video>
        <div class="clear"></div>
        <h2 class="subtitle has-text-centered">
        </h2>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/Nikon_Sony_EG1_LR.png" alt="MY ALT TEXT"/>
        <br/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison with baselines for Sony-source and Nikon-target images.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/Canon_Sony_EG2_v2_LR.png" alt="MY ALT TEXT"/>
        <br/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison with baselines for Sony-source and Canon-target images.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{prabhakar2021fewshot,
        title     = {Few-Shot Domain Adaptation for Low Light RAW Image Enhancement},
        author    = {K. Prabhakar and Vishal Vinod and N. Sahoo and Venkatesh Babu Radhakrishnan},
        journal   = {British Machine Vision Conference},
        year      = {2021},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
